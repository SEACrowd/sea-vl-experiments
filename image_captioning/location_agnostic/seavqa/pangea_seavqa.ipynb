{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343832e-8f44-4be6-bb24-b5fbf336728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%cd ~\n",
    "if not os.path.exists(\"LLaVA-NeXT\"):\n",
    "    !git clone https://github.com/LLaVA-VL/LLaVA-NeXT\n",
    "%cd /LLaVA-NeXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e840d465-6622-490a-8c80-3191fc8ad349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\n",
    "from typing import Dict\n",
    "from PIL import Image\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e17326e3-fb2e-4bf8-9a62-40b077bd6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_image_iterator(root_dir):\n",
    "    images_dir = os.path.join(root_dir, \"images\")\n",
    "\n",
    "    for json_file in os.listdir(root_dir):\n",
    "        if json_file.endswith(\".json\"):\n",
    "            json_path = os.path.join(root_dir, json_file)\n",
    "            \n",
    "            with open(json_path, \"r\") as f:\n",
    "                data_list = json.load(f)\n",
    "\n",
    "            file_name = os.path.splitext(json_file)[0]\n",
    "            \n",
    "            for data in data_list:\n",
    "                image_filename = data[\"image_path\"].split(\"/\")[-1]\n",
    "                \n",
    "                image_found = False\n",
    "                for root, dirs, files in os.walk(images_dir):\n",
    "                    if image_filename in files:\n",
    "                        image_path = os.path.join(root, image_filename)\n",
    "                        \n",
    "                        try:\n",
    "                            # image = Image.open(image_path)\n",
    "                            yield file_name, data, image_path \n",
    "                            image_found = True\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error opening image {image_path}: {e}\")\n",
    "                        break\n",
    "                \n",
    "                if not image_found:\n",
    "                    print(f\"Image not found for entry in {json_file}: {image_filename}\")\n",
    "                    yield file_name, data, None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74917f73-c904-4fe3-9f41-7be9e0e2200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_prompt = '''Write a caption in English for an image that may include culturally significant objects or elements from Southeast Asia.  \n",
    "The caption should specifically name Southeast Asian cultural items, such as cuisine, traditions, landmarks, or other related elements if they appear in the image.\n",
    "The caption should be concise, consisting of 3 to 5 sentences.'''\n",
    "save_path= \"pangea_seavqa_en_result.json\"\n",
    "image_root_path = r\"/root/filter_sea_vqa_final/filter_sea_vqa_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681abb3c-fcaf-4fac-8377-6ba0af8a7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'neulab/Pangea-7B'\n",
    "model_name = 'Pangea-7B-qwen'\n",
    "args = {\"multimodal\": True}\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name, torch_dtype=\"float16\", **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374457fd-7c5c-4f94-bdae-080c9ee4f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=512, system_message: str = \"You are a helpful assistant.\") -> Dict:\n",
    "    roles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\n",
    "    im_start, im_end = tokenizer.additional_special_tokens_ids\n",
    "    nl_tokens = tokenizer(\"\\n\").input_ids\n",
    "    _system = tokenizer(\"system\").input_ids + nl_tokens\n",
    "    _user = tokenizer(\"user\").input_ids + nl_tokens\n",
    "    _assistant = tokenizer(\"assistant\").input_ids + nl_tokens\n",
    "    input_ids = []\n",
    "    source = sources\n",
    "    if roles[source[0][\"from\"]] != roles[\"human\"]: source = source[1:]\n",
    "    input_id, target = [], []\n",
    "    system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n",
    "    input_id += system\n",
    "    target += [im_start] + [IGNORE_INDEX] * (len(system) - 3) + [im_end] + nl_tokens\n",
    "    assert len(input_id) == len(target)\n",
    "    for j, sentence in enumerate(source):\n",
    "        role = roles[sentence[\"from\"]]\n",
    "        if has_image and sentence[\"value\"] is not None and \"<image>\" in sentence[\"value\"]:\n",
    "            num_image = len(re.findall(DEFAULT_IMAGE_TOKEN, sentence[\"value\"]))\n",
    "            texts = sentence[\"value\"].split('<image>')\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens \n",
    "            for i,text in enumerate(texts):\n",
    "                _input_id += tokenizer(text).input_ids \n",
    "                if i<len(texts)-1: _input_id += [IMAGE_TOKEN_INDEX] + nl_tokens\n",
    "            _input_id += [im_end] + nl_tokens\n",
    "            assert sum([i==IMAGE_TOKEN_INDEX for i in _input_id])==num_image\n",
    "        else:\n",
    "            if sentence[\"value\"] is None: _input_id = tokenizer(role).input_ids + nl_tokens\n",
    "            else: _input_id = tokenizer(role).input_ids + nl_tokens + tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n",
    "        input_id += _input_id\n",
    "    input_ids.append(input_id)\n",
    "    return torch.tensor(input_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9913a2-aec5-4a19-9390-efaa804990a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption(image, prompt):\n",
    "    image_tensors = []\n",
    "    prompt = \"<image>\\n\" + prompt\n",
    "\n",
    "    image = Image.open(image)\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values']\n",
    "    image_tensors.append(image_tensor.half().cuda())\n",
    "    input_ids = preprocess_qwen([{'from': 'human', 'value': prompt},{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensors,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=True\n",
    "        )\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    outputs = outputs.strip()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec8f50b5-4436-4051-9435-9fe38c0cbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(save_path, json_obj):\n",
    "    if not os.path.exists(save_path):\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump([json_obj], f, indent=4)\n",
    "    else:\n",
    "        with open(save_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        data.append(json_obj)\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d9d69-7cf6-4b61-b432-c240659f0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = json_image_iterator(image_root_path)\n",
    "items = list(iterator)\n",
    "\n",
    "for country, data, image in tqdm(items, desc=\"Progess\"):\n",
    "    caption = get_caption(image, en_prompt)\n",
    "    caption = caption.replace('Caption: ', '').replace('\"','').replace('The caption for this image could be:','')\n",
    "\n",
    "    json_obj={\"name\":data[\"culture_name\"],\n",
    "              \"country\":country,\n",
    "              \"image_url\":data[\"image_path\"],\n",
    "              \"gt_caption\":data[\"gt_caption\"],\n",
    "              \"caption\":caption}\n",
    "            \n",
    "    save_to_json(save_path, json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51514c11-8567-4f57-aa72-787e6b26a419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
