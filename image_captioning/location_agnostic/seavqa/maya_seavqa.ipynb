{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58112b2-6815-4f81-947c-c0f609f9416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%cd ~\n",
    "if not os.path.exists(\"maya\"):\n",
    "    !git clone https://github.com/nahidalam/maya\n",
    "%cd maya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e840d465-6622-490a-8c80-3191fc8ad349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "from llava.conversation import conv_templates\n",
    "from llava.eval.maya.eval_utils import load_maya_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from PIL import Image\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6662826d-f7a0-4b60-85aa-1326d0b5088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_image_iterator(root_dir):\n",
    "    images_dir = os.path.join(root_dir, \"images\")\n",
    "\n",
    "    for json_file in os.listdir(root_dir):\n",
    "        if json_file.endswith(\".json\"):\n",
    "            json_path = os.path.join(root_dir, json_file)\n",
    "            \n",
    "            with open(json_path, \"r\") as f:\n",
    "                data_list = json.load(f)\n",
    "\n",
    "            file_name = os.path.splitext(json_file)[0]\n",
    "            \n",
    "            for data in data_list:\n",
    "                image_filename = data[\"image_path\"].split(\"/\")[-1]\n",
    "                \n",
    "                image_found = False\n",
    "                for root, dirs, files in os.walk(images_dir):\n",
    "                    if image_filename in files:\n",
    "                        image_path = os.path.join(root, image_filename)\n",
    "                        \n",
    "                        try:\n",
    "                            # image = Image.open(image_path)\n",
    "                            yield file_name, data, image_path \n",
    "                            image_found = True\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error opening image {image_path}: {e}\")\n",
    "                        break\n",
    "                \n",
    "                if not image_found:\n",
    "                    print(f\"Image not found for entry in {json_file}: {image_filename}\")\n",
    "                    yield file_name, data, None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74917f73-c904-4fe3-9f41-7be9e0e2200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_prompt = '''Write a caption in English for an image that may include culturally significant objects or elements from Southeast Asia.  \n",
    "The caption should specifically name Southeast Asian cultural items, such as cuisine, traditions, landmarks, or other related elements if they appear in the image.\n",
    "The caption should be concise, consisting of 3 to 5 sentences.'''\n",
    "save_path= \"maya_seavqa_en_result.json\"\n",
    "image_root_path = r\"/root/filter_sea_vqa_final/filter_sea_vqa_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7004d4d-78dc-406c-96e6-76ba8377abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = \"CohereForAI/aya-23-8B\"\n",
    "model_path = \"maya-multimodal/maya\"\n",
    "mode = \"finetuned\"  \n",
    "projector_path = None  \n",
    "model, tokenizer, image_processor, _ = load_maya_model(\n",
    "        model_base, model_path, projector_path if mode == \"pretrained\" else None, mode\n",
    "    )\n",
    "model = model.half().cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9913a2-aec5-4a19-9390-efaa804990a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption(image, prompt):\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        prompt = (\n",
    "            DEFAULT_IM_START_TOKEN\n",
    "            + DEFAULT_IMAGE_TOKEN\n",
    "            + DEFAULT_IM_END_TOKEN\n",
    "            + \"\\n\"\n",
    "            + prompt\n",
    "        )\n",
    "    else:\n",
    "        prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "    conv = conv_templates[\"aya\"].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda()\n",
    "    )\n",
    "\n",
    "    image = Image.open(image).convert(\"RGB\")\n",
    "    image_tensor = process_images([image], image_processor, model.config)[0]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor.unsqueeze(0).half().cuda(),\n",
    "            image_sizes=[image.size],\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        \n",
    "    answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8f50b5-4436-4051-9435-9fe38c0cbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(save_path, json_obj):\n",
    "    if not os.path.exists(save_path):\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump([json_obj], f, indent=4)\n",
    "    else:\n",
    "        with open(save_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        data.append(json_obj)\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d9d69-7cf6-4b61-b432-c240659f0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = json_image_iterator(image_root_path)\n",
    "items = list(iterator)\n",
    "\n",
    "for country, data, image in tqdm(items, desc=\"Progess\"):\n",
    "    caption = get_caption(image, en_prompt)\n",
    "    \n",
    "    json_obj={\"name\":data[\"culture_name\"],\n",
    "              \"country\":country,\n",
    "              \"image_url\":data[\"image_path\"],\n",
    "              \"gt_caption\":data[\"gt_caption\"],\n",
    "              \"caption\":caption}\n",
    "            \n",
    "    save_to_json(save_path, json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916e51e-e2fa-421e-be5a-aeff46b1cb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
